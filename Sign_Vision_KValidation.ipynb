{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m f1_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Iterate through each fold\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, val_index \u001b[38;5;129;01min\u001b[39;00m kf\u001b[38;5;241m.\u001b[39msplit(\u001b[43mx_train\u001b[49m, y_train):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Split the data into training and validation sets based on current fold\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     x_fold_train, x_fold_val \u001b[38;5;241m=\u001b[39m x_train[train_index], x_train[val_index]\n\u001b[0;32m     18\u001b[0m     y_fold_train, y_fold_val \u001b[38;5;241m=\u001b[39m y_train[train_index], y_train[val_index]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing necessary library\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Define the number of splits for K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store performance metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Iterate through each fold\n",
    "for train_index, val_index in kf.split(x_train, y_train):\n",
    "    # Split the data into training and validation sets based on current fold\n",
    "    x_fold_train, x_fold_val = x_train[train_index], x_train[val_index]\n",
    "    y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Reinitialize the model for each fold\n",
    "    fold_model = Sequential()\n",
    "    fold_model.add(Conv2D(75, (3,3), strides=1, padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    fold_model.add(BatchNormalization())\n",
    "    fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "    fold_model.add(Conv2D(50, (3,3), strides=1, padding='same', activation='relu'))\n",
    "    fold_model.add(Dropout(0.2))\n",
    "    fold_model.add(BatchNormalization())\n",
    "    fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "    fold_model.add(Conv2D(25, (3,3), strides=1, padding='same', activation='relu'))\n",
    "    fold_model.add(BatchNormalization())\n",
    "    fold_model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "    fold_model.add(Flatten())\n",
    "    fold_model.add(Dense(512, activation='relu'))\n",
    "    fold_model.add(Dropout(0.3))\n",
    "    fold_model.add(Dense(24, activation='softmax'))\n",
    "    fold_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Train the model on the training fold data\n",
    "    fold_model.fit(datagen.flow(x_fold_train, y_fold_train, batch_size=128),\n",
    "               epochs=10,\n",
    "               validation_data=(x_fold_val, y_fold_val),\n",
    "               callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Evaluate the model on the validation set for the current fold\n",
    "    predictions = fold_model.predict(x_fold_val)\n",
    "    y_fold_pred = np.argmax(predictions, axis=1)\n",
    "    y_fold_true = np.argmax(y_fold_val, axis=1)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_fold_true, y_fold_pred)\n",
    "    precision = precision_score(y_fold_true, y_fold_pred, average='macro')\n",
    "    recall = recall_score(y_fold_true, y_fold_pred, average='macro')\n",
    "    f1 = f1_score(y_fold_true, y_fold_pred, average='macro')\n",
    "\n",
    "    # Store the performance metrics for the current fold\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate the mean and standard deviation of the performance metrics across all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "std_recall = np.std(recalls)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance metrics for the model\n",
    "print(\"Mean Accuracy of model: {:.2f}%\".format(mean_accuracy * 100))\n",
    "print(\"Mean Precision of model: {:.2f}%\".format(mean_precision * 100))\n",
    "print(\"Mean Recall of model: {:.2f}%\".format(mean_recall * 100))\n",
    "print(\"Mean F1-Score of model: {:.2f}%\".format(mean_f1 * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
